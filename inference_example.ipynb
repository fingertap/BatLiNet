{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Using BatLiNet for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "import hashlib\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(str(Path.cwd()))\n",
    "from src.task import Task\n",
    "from src.builders import MODELS\n",
    "from src.utils import import_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiment, there are two methods to train a model.\n",
    "\n",
    "Method 1: Call the wrapped main function. This involves directly invoking the main function within the encapsulated 'pipeline.py' for both training and evaluation.\n",
    "\n",
    "Method 2: Pipeline details. This method involves gradually unfolding the contents within the pipeline, such as loading configurations, building the dataset, training, predicting, and so forth.\n",
    "\n",
    "You are free to choose either of these methods to reproduce the code as per your convenience and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Calling the wrapped main function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.pipeline import main\n",
    "for seed in range(8):\n",
    "    config_path = \"./configs/ablation/diff_branch/batlinet/mix_100.yaml\"\n",
    "    workspace = \"./workspaces/ablation/diff_branch/batlinet/mix_100\"\n",
    "    # If train is true, the model needs to be trained from scratch, if it is false it will be loaded from checkpoint\n",
    "    main(config_path=config_path, workspace=workspace, seed=seed, train=False, evaluate=True, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Pipeline details"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define helper functions\n",
    "We first define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use these functions to name the dumped files\n",
    "def hash_string(string):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    sha256_hash.update(string.encode('utf-8'))\n",
    "    hash_value = sha256_hash.hexdigest()\n",
    "    truncated_hash = hash_value[:32]\n",
    "    return truncated_hash\n",
    "\n",
    "\n",
    "def timestamp(marker: bool = False):\n",
    "    template = '%Y-%m-%d %H:%M:%S' if marker else '%Y%m%d%H%M%S'\n",
    "    return datetime.now().strftime(template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed\n",
    "We set the seed of the experiment with the following function. However, some low-level code may still bring in randomness, which may slightly influence the final scores (<10 RMSE, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    print(f'Seed is set to {seed}.')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config\n",
    "We use config files to organize our experiments. We will use the following function to load the config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = [\n",
    "    'model',\n",
    "    'train_test_split',\n",
    "    'feature',\n",
    "    'label',\n",
    "    'feature_transformation',\n",
    "    'label_transformation'\n",
    "]\n",
    "\n",
    "\n",
    "def load_config(config_path: str, workspace: str) -> dict:\n",
    "    config_path = Path(config_path)\n",
    "    configs = import_config(config_path, CONFIGS)\n",
    "\n",
    "    # Determine the workspace\n",
    "    if configs['model'].get('workspace') is not None:\n",
    "        workspace = Path(configs['model'].get('workspace'))\n",
    "    elif workspace is not None:\n",
    "        if workspace.strip().lower() == 'none':\n",
    "            workspace = None\n",
    "        else:\n",
    "            workspace = Path(workspace)\n",
    "    else:\n",
    "        workspace = Path.cwd() / 'workspaces' / config_path.stem\n",
    "        warnings.warn(f'Setting workspace to {str(workspace)}. If you '\n",
    "                       'do not want any information to be stored, '\n",
    "                       'explicitly call with flag `--workspace none`.')\n",
    "\n",
    "    if workspace is not None and workspace.exists():\n",
    "        assert workspace.is_dir(), workspace\n",
    "\n",
    "    if workspace is not None and not workspace.exists():\n",
    "        os.makedirs(workspace)\n",
    "\n",
    "    configs['workspace'] = workspace\n",
    "\n",
    "    return configs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset\n",
    "As the preprocessing of the datasets are time-consuming, we cache the preprocessed data to save both time and memory (when using parallel computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_dump_string(data):\n",
    "    if isinstance(data, list):\n",
    "        return '_'.join([recursive_dump_string(x) for x in data])\n",
    "    if isinstance(data, dict):\n",
    "        return '_'.join([\n",
    "            recursive_dump_string(data[key])\n",
    "            for key in sorted(data.keys())\n",
    "        ])\n",
    "    return str(data)\n",
    "\n",
    "\n",
    "def build_dataset(configs: dict, device: str):\n",
    "    strings = []\n",
    "    fields = ['label', 'feature', 'train_test_split',\n",
    "              'feature_transformation', 'label_transformation']\n",
    "    for field in fields:\n",
    "        strings.append(recursive_dump_string(configs[field]))\n",
    "    filename = hash_string('+'.join(strings))\n",
    "    cache_dir = Path('cache')\n",
    "    if not cache_dir.exists():\n",
    "        cache_dir.mkdir()\n",
    "    cache_file = Path(cache_dir / f'battery_cache_{filename}.pkl')\n",
    "\n",
    "    if cache_file.exists():\n",
    "        warnings.warn(f'Load datasets from cache {str(cache_file)}.')\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "    else:\n",
    "        dataset = Task(\n",
    "            label_annotator=configs['label'],\n",
    "            feature_extractor=configs['feature'],\n",
    "            train_test_splitter=configs['train_test_split'],\n",
    "            feature_transformation=configs['feature_transformation'],\n",
    "            label_transformation=configs['label_transformation']).build()\n",
    "        # store cache\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "    return dataset.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate \n",
    "The following is the main logic of evaluation. We load in the correct config file and then train or evaluate the model to obtain metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"./configs/ablation/diff_branch/batlinet/mix_100.yaml\")\n",
    "workspace = Path(\"./workspaces/ablation/diff_branch/batlinet/mix_100\")\n",
    "configs = load_config(config_path, workspace)\n",
    "metric = ['RMSE', 'MAE', 'MAPE']\n",
    "device = 'cpu'\n",
    "train_from_scratch = False  # Whether we train the model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test 8 seeds\n",
    "for seed in range(8):\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Prepare dataset\n",
    "    dataset = build_dataset(configs, device).to(device)\n",
    "\n",
    "    # Model preparation\n",
    "    configs['model']['seed'] = seed\n",
    "    model = MODELS.build(configs['model'])\n",
    "    \n",
    "    if not train_from_scratch:\n",
    "        # load model from checkpoint\n",
    "        checkpoint = next(workspace.glob(f'*seed_{seed}*'))\n",
    "        model.load_checkpoint(checkpoint)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Store the current config to workspace\n",
    "    ts = timestamp()\n",
    "    if model.workspace is not None:\n",
    "        shutil.copyfile(config_path, model.workspace / f'config_{ts}.yaml')\n",
    "\n",
    "    if train_from_scratch:\n",
    "        # train from scratch\n",
    "        model.fit(dataset, timestamp=ts)\n",
    "\n",
    "    # Evaluate\n",
    "    prediction = model.predict(dataset)\n",
    "    scores = {\n",
    "        m: dataset.evaluate(prediction, m) for m in metric\n",
    "    }\n",
    "\n",
    "    # Save predictions\n",
    "    if model.workspace is not None:\n",
    "        obj = {\n",
    "            'prediction': prediction,\n",
    "            'scores': scores,\n",
    "            'data': dataset.to('cpu'),\n",
    "            'seed': seed,\n",
    "        }\n",
    "        with open(\n",
    "            model.workspace / f'predictions_seed_{seed}_{ts}.pkl', 'wb'\n",
    "        ) as f:\n",
    "            pickle.dump(obj, f)\n",
    "\n",
    "    # Print metrics\n",
    "    print(' '.join([\n",
    "        f'{m}: {s:.2f}' for m, s in scores.items()\n",
    "    ]), flush=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the complete reproduction of our experiments, please refer to our [detailed guide](README.md). Also, we visualized all the tables and figures [here](notebooks)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
