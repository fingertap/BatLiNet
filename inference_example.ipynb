{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Using BatLiNet for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "import hashlib\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(str(Path.cwd()))\n",
    "from src.task import Task\n",
    "from src.builders import MODELS\n",
    "from src.utils import import_config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use these functions to name the dumped files\n",
    "def hash_string(string):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    sha256_hash.update(string.encode('utf-8'))\n",
    "    hash_value = sha256_hash.hexdigest()\n",
    "    truncated_hash = hash_value[:32]\n",
    "    return truncated_hash\n",
    "\n",
    "\n",
    "def timestamp(marker: bool = False):\n",
    "    template = '%Y-%m-%d %H:%M:%S' if marker else '%Y%m%d%H%M%S'\n",
    "    return datetime.now().strftime(template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the seed of the experiment with the following function. However, some low-level code may still bring in randomness, which may slightly influence the final scores (<10 RMSE, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    print(f'Seed is set to {seed}.')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use config files to organize our experiments. We will use the following function to load the config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = [\n",
    "    'model',\n",
    "    'train_test_split',\n",
    "    'feature',\n",
    "    'label',\n",
    "    'feature_transformation',\n",
    "    'label_transformation'\n",
    "]\n",
    "\n",
    "\n",
    "def load_config(config_path: str, workspace: str) -> dict:\n",
    "    config_path = Path(config_path)\n",
    "    configs = import_config(config_path, CONFIGS)\n",
    "\n",
    "    # Determine the workspace\n",
    "    if configs['model'].get('workspace') is not None:\n",
    "        workspace = Path(configs['model'].get('workspace'))\n",
    "    elif workspace is not None:\n",
    "        if workspace.strip().lower() == 'none':\n",
    "            workspace = None\n",
    "        else:\n",
    "            workspace = Path(workspace)\n",
    "    else:\n",
    "        workspace = Path.cwd() / 'workspaces' / config_path.stem\n",
    "        warnings.warn(f'Setting workspace to {str(workspace)}. If you '\n",
    "                       'do not want any information to be stored, '\n",
    "                       'explicitly call with flag `--workspace none`.')\n",
    "\n",
    "    if workspace is not None and workspace.exists():\n",
    "        assert workspace.is_dir(), workspace\n",
    "\n",
    "    if workspace is not None and not workspace.exists():\n",
    "        os.makedirs(workspace)\n",
    "\n",
    "    configs['workspace'] = workspace\n",
    "\n",
    "    return configs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the preprocessing of the datasets are time-consuming, we cache the preprocessed data to save both time and memory (when using parallel computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_dump_string(data):\n",
    "    if isinstance(data, list):\n",
    "        return '_'.join([recursive_dump_string(x) for x in data])\n",
    "    if isinstance(data, dict):\n",
    "        return '_'.join([\n",
    "            recursive_dump_string(data[key])\n",
    "            for key in sorted(data.keys())\n",
    "        ])\n",
    "    return str(data)\n",
    "\n",
    "\n",
    "def build_dataset(configs: dict, device: str):\n",
    "    strings = []\n",
    "    fields = ['label', 'feature', 'train_test_split',\n",
    "              'feature_transformation', 'label_transformation']\n",
    "    for field in fields:\n",
    "        strings.append(recursive_dump_string(configs[field]))\n",
    "    filename = hash_string('+'.join(strings))\n",
    "    cache_dir = Path('cache')\n",
    "    if not cache_dir.exists():\n",
    "        cache_dir.mkdir()\n",
    "    cache_file = Path(cache_dir / f'battery_cache_{filename}.pkl')\n",
    "\n",
    "    if cache_file.exists():\n",
    "        warnings.warn(f'Load datasets from cache {str(cache_file)}.')\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "    else:\n",
    "        dataset = Task(\n",
    "            label_annotator=configs['label'],\n",
    "            feature_extractor=configs['feature'],\n",
    "            train_test_splitter=configs['train_test_split'],\n",
    "            feature_transformation=configs['feature_transformation'],\n",
    "            label_transformation=configs['label_transformation']).build()\n",
    "        # store cache\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "    return dataset.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the main logic of training and evaluation. We load in the correct config file and then train or evaluate the model to obtain metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config_path: str,\n",
    "         workspace: str = None,\n",
    "         train: bool = False,\n",
    "         evaluate: bool = True,\n",
    "         checkpoint: str = None,\n",
    "         device: str = 'cpu',\n",
    "         metric: str = 'RMSE,MAE,MAPE',\n",
    "         skip_training_if_exists: bool = False,\n",
    "         skip_evaluation_if_exists: bool = False,\n",
    "         build_cache_only: bool = False,\n",
    "         seed: int = 0,\n",
    "         epochs: int = None):\n",
    "    set_seed(seed)\n",
    "    configs = load_config(config_path, workspace)\n",
    "\n",
    "    if build_cache_only:\n",
    "        dataset = build_dataset(configs, device)\n",
    "        return\n",
    "\n",
    "    if isinstance(metric, str):\n",
    "        metric = metric.split(',')\n",
    "\n",
    "    dataset = None\n",
    "\n",
    "    if epochs is not None:\n",
    "        configs['model']['epochs'] = epochs\n",
    "\n",
    "    configs['model']['seed'] = seed\n",
    "\n",
    "    model = MODELS.build(configs['model'])\n",
    "    if model.workspace is None:\n",
    "        model.workspace = configs['workspace']\n",
    "\n",
    "    if checkpoint is not None:\n",
    "        model.load_checkpoint(checkpoint)\n",
    "\n",
    "    if torch.__version__ >= '2' and isinstance(model, torch.nn.Module):\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    ts = timestamp()\n",
    "    if model.workspace is not None:\n",
    "        shutil.copyfile(config_path, model.workspace / f'config_{ts}.yaml')\n",
    "\n",
    "    if train:\n",
    "        if skip_training_if_exists and model.workspace is not None and \\\n",
    "                (model.workspace / 'latest.ckpt').exists():\n",
    "            warnings.warn(f'Skip training for {configs[\"workspace\"]} '\n",
    "                           'as the checkpoint exists.')\n",
    "        else:\n",
    "            if dataset is None:\n",
    "                dataset = build_dataset(configs, device)\n",
    "            model.fit(dataset, timestamp=ts)\n",
    "\n",
    "    if evaluate:\n",
    "        # Evaluate and save predictions\n",
    "        if skip_evaluation_if_exists and model.workspace is not None and \\\n",
    "                any(model.workspace.glob('predictions_*.pkl')):\n",
    "            warnings.warn(f'Skip evaluation for {configs[\"workspace\"]} '\n",
    "                           'as the prediction exists.')\n",
    "        else:\n",
    "            if dataset is None:\n",
    "                dataset = build_dataset(configs, device)\n",
    "            prediction = model.predict(dataset)\n",
    "            scores = {\n",
    "                m: dataset.evaluate(prediction, m) for m in metric\n",
    "            }\n",
    "            if model.workspace is not None:\n",
    "                obj = {\n",
    "                    'prediction': prediction,\n",
    "                    'scores': scores,\n",
    "                    'data': dataset.to('cpu'),\n",
    "                    'seed': seed,\n",
    "                }\n",
    "                with open(\n",
    "                    model.workspace / f'predictions_seed_{seed}_{ts}.pkl', 'wb'\n",
    "                ) as f:\n",
    "                    pickle.dump(obj, f)\n",
    "\n",
    "            # Print metrics\n",
    "            print(\n",
    "                ' '.join([f'{m}: {s:.2f}' for m, s in scores.items()]),\n",
    "                flush=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the checkpoints for inference under 8 different seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(8):\n",
    "    config_path = \"./configs/ablation/diff_branch/batlinet/mix_100.yaml\"\n",
    "    workspace = \"./workspaces/ablation/diff_branch/batlinet/mix_100\"\n",
    "    main(config_path=config_path, workspace=workspace, seed=seed, train=False, evaluate=True, device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
